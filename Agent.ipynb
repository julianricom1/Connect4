{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caca4075-ec01-443d-b49b-b84f0ea798aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition on the Agent that plays Connect4\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    import Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68cc4297-5b22-4c9d-99aa-213f94b01ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env ,chip, epsilon = 0.95, alpha = 0.5, gamma = 1, filename = ''):\n",
    "        # '''\n",
    "        # reward_scheme <(float, float, float, float)> : (reward for a move that doesn´t end the game, reward for losing, reward fora tied game, reward for winning)\n",
    "        # chip <string> = chip to be played by the agent. must be either \"X\" or \"O\"\n",
    "        # \n",
    "        # '''\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.state = []\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "     \n",
    "        if chip not in [\"X\", \"O\"]:\n",
    "            raise ValueError(\"chip must be 'X' or 'O'\")\n",
    "        self.chip = chip\n",
    "\n",
    "        if filename != '':\n",
    "            self.filename = filename\n",
    "            self.q_table = self.import_Qtable(filename)\n",
    "        else:\n",
    "            self.q_table = {} # La tabla que hay que exportar y leer. self.import_Qtable(self.filename)\n",
    "\n",
    "    def import_Qtable(self, filename):\n",
    "        try:\n",
    "            print(filename)\n",
    "    \n",
    "            with open(filename, 'rb') as file:\n",
    "                q_table = pickle.load(file)\n",
    "                print(\"Q-table load successfull new\")\n",
    "                print(q_table)\n",
    "                return q_table\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Starting with an empty Q-Table.\")\n",
    "        except pickle.UnpicklingError:\n",
    "            print(\"Error: The file content is not a valid pickle format.\")\n",
    "        except EOFError:\n",
    "            print(\"Error: The file is incomplete or corrupted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "    def export_Qtable(self,filename):\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(self.q_table, file)\n",
    "\n",
    "    \n",
    "    def export_Qtable_JSON(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'w') as file:\n",
    "                # Convertir claves a cadenas para guardarlas en JSON\n",
    "                raw_data = {str(key): value for key, value in self.q_table.items()}\n",
    "                json.dump(raw_data, file, indent=4)\n",
    "                print(\"Q-table saved successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving Q-table: {e}\")\n",
    "\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        state = []\n",
    "        for column in range(self.env.columns):\n",
    "            vectors =self.env.get_vectors_of_column(column)\n",
    "            max_own = 0\n",
    "            max_other = 0\n",
    "            if(vectors[0] == [] and vectors[1] == [] and vectors[2] == [] and vectors[3] == []):\n",
    "                state.append((-1, -1))\n",
    "            else :\n",
    "                for index, vector in enumerate(vectors):\n",
    "                    result = self.env.verify_vector(vector[0], vector[1], self.chip)\n",
    "                    max_own = max(max_own, result[0])\n",
    "                    max_other = max(max_other, result[0])\n",
    "                state.append((max_own, max_other))\n",
    "        return state\n",
    "\n",
    "    def get_value(self, state, action:int) -> float:\n",
    "        if self.q_table.get((tuple(state),action)) == None:\n",
    "            return 0\n",
    "        return self.q_table.get((tuple(state),action))\n",
    "    \n",
    "    def best_action(self, state) -> int:\n",
    "        actions = self.env.get_possible_actions()\n",
    "        values = {}\n",
    "    \n",
    "        if len(actions) == 0:\n",
    "            return -1\n",
    "    \n",
    "        for action in actions:\n",
    "            values[(tuple(state), action)] = self.get_value(state, action)\n",
    "    \n",
    "        best_state, best_action = max(values, key=values.get)\n",
    "    \n",
    "        # Filtra acciones empatadas en el mejor valor\n",
    "        filtered_values = {key: val for key, val in values.items() if val == values[(best_state, best_action)]}\n",
    "    \n",
    "        # Selecciona aleatoriamente entre las mejores acciones empatadas\n",
    "        best_tuple, _ = random.choice(list(filtered_values.items()))\n",
    "        return best_tuple[1]\n",
    "\n",
    "    \n",
    "    def choose_action(self, state) -> int:\n",
    "        actions = self.env.get_possible_actions()\n",
    "        action = -1\n",
    "        prob = random.uniform(0,1)\n",
    "        if prob <= self.epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = self.best_action(state)\n",
    "        return action\n",
    "    \n",
    "    def update_values(self, state, action: int, next_state, reward: int) -> None:\n",
    "        actual_Q_value = self.get_value(state, action)\n",
    "        next_action = self.best_action(next_state)\n",
    "        next_Q_value = self.get_value(next_state, next_action)\n",
    "        new_Q_value = ((1 - self.alpha) * actual_Q_value) + self.alpha * (reward + (self.gamma * next_Q_value))\n",
    "        self.q_table[(tuple(state), action)] = new_Q_value\n",
    "\n",
    "\n",
    "    def step(self, state, action:int) -> tuple[tuple[int,int],int, bool, str]:\n",
    "        own_neighbors, opponent_neighbors = state[action - 1]        \n",
    "        # Aca se define la estrategia a tomar, si es defensiva u ofensiva\n",
    "        own_neighbors_rewards = [0,20,50,200,500] # Son las recompensas dadas por la cantidad de vecinos propios\n",
    "        opponent_neighbors_rewards = [0,5,15,70,-100] # Son las recompensas dadas por la cantidad de vecinos oponentes\n",
    "\n",
    "        reward = own_neighbors_rewards[own_neighbors] + opponent_neighbors_rewards[opponent_neighbors]\n",
    "        self.env.place_chip(action, self.chip)\n",
    "        status= self.env.verify_winner(self.chip)\n",
    "        info = ''\n",
    "        if status : \n",
    "            info = 'El estado es terminal'\n",
    "        else :\n",
    "            info = 'El juego continua'\n",
    "        return (reward, status, info)\n",
    "\n",
    "    \n",
    "    def play_turn(self, episode = 1):\n",
    "            \n",
    "        if self.state == []:\n",
    "            self.state = self.get_current_state()\n",
    "            self.action = self.choose_action(self.state)\n",
    "        else:\n",
    "            next_state = self.get_current_state()\n",
    "            next_action = self.choose_action(next_state)\n",
    "            self.update_values(self.state, self.action, next_state,self.reward)\n",
    "            self.state, self.action = next_state, next_action\n",
    "\n",
    "        self.reward, done, info = self.step(self.state, self.action)\n",
    "        \n",
    "\n",
    "        if ((episode+1) % 200) == 0:\n",
    "            if self.epsilon > 0.01:                    \n",
    "                self.epsilon -= (self.epsilon*0.1)\n",
    "        \n",
    "    def test_performance(self) -> tuple[dict, dict]:\n",
    "        actions = {}\n",
    "        values = {}\n",
    "        for i in range(self.env.nrows):\n",
    "            for j in range(self.env.ncols):\n",
    "                state = (i, j)  # Asegúrate de que `state` sea hashable\n",
    "                if not self.env.is_terminal(state):\n",
    "                    action = self.best_action(state)\n",
    "                    actions[(i, j)] = action\n",
    "                    values[(tuple(state), action)] = self.get_value(state, action)\n",
    "        return actions, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35066b1a-a746-4a76-b6ef-558c8c7aad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = [\n",
    "            ['-','-','-','-','-','-','-'],\n",
    "            ['-','-','-','-','-','-','-'],\n",
    "            ['-','-','O','-','-','-','-'],\n",
    "            ['-','-','O','-','-','-','-'],\n",
    "            ['O','-','X','X','X','X','O'],\n",
    "            ['-','-','O','O','X','O','X']\n",
    "          ]\n",
    "    agente = Agent(env,'X')\n",
    "    \n",
    "    vector = ['-','X','X','-','X','O']\n",
    "    assert agente.verify_vector(3,vector,'X') == (3,0), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','O','O','-','X','O']\n",
    "    assert agente.verify_vector(3,vector,'X') == (1,2), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','O','O','-','X','O']\n",
    "    assert agente.verify_vector(0,vector,'X') == (0,2), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','O','O','X','X','-']\n",
    "    assert agente.verify_vector(5,vector,'X') == (2,0), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','-','-','-']\n",
    "    assert agente.verify_vector(2,vector,'X') == (0,0), \"La funcion no esta retornando los valores correctos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc46648c-4a8b-4910-b57d-83fc97f1aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crear una instancia del agente\n",
    "# agent = Connect4Agent(chip=\"X\", reward_scheme=(0.0, -1.0, 0.5, 1.0))\n",
    "\n",
    "# # Ejemplo de un tablero de juego\n",
    "# board = [\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"X\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"X\", \"O\", \"-\", \"-\"],\n",
    "#     [\"X\", \"X\", \"X\", \"O\", \"X\", \"-\", \"-\"],\n",
    "#     [\"O\", \"X\", \"O\", \"X\", \"O\", \"-\", \"-\"]\n",
    "# ]\n",
    "\n",
    "# # Escoger una acción con un 10% de exploración y 90% de explotación\n",
    "# epsilon = 0.1  # Tasa de exploración\n",
    "# action = agent.choose_action(board, epsilon)\n",
    "# print(\"Chosen action (column):\", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df385d43-037f-438c-84d1-205a37c5cff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
