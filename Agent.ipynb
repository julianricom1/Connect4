{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caca4075-ec01-443d-b49b-b84f0ea798aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna inv√°lida. Elija un n√∫mero entre 1 y 7.\n",
      "Columna inv√°lida. Elija un n√∫mero entre 1 y 7.\n",
      "Ficha no valida\n",
      "Columna llena. Elija otra columna.\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Definition on the Agent that plays Connect4\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    import Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cc4297-5b22-4c9d-99aa-213f94b01ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env ,chip, reward_scheme = (0.0, -1.0, 0.5, 1.0), epsilon = 0.95 filename = ''):\n",
    "        # '''\n",
    "        # reward_scheme <(float, float, float, float)> : (reward for a move that doesn¬¥t end the game, reward for losing, reward fora tied game, reward for winning)\n",
    "        # chip <string> = chip to be played by the agent. must be either \"X\" or \"O\"\n",
    "        # \n",
    "        # '''\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.state = []\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        # Verificar que reward_scheme es una tupla de 4 floats\n",
    "        if not (isinstance(reward_scheme, tuple) and len(reward_scheme) == 4 and all(isinstance(x, float) for x in reward_scheme)):\n",
    "            raise ValueError(\"reward_scheme must be a tupla with 4 floats\")\n",
    "        self.rewards = reward_scheme\n",
    "     \n",
    "        if chip not in [\"X\", \"O\"]:\n",
    "            raise ValueError(\"chip must be 'X' or 'O'\")\n",
    "        self.chip = chip\n",
    "\n",
    "        if filename != '':\n",
    "            self.filename = filename\n",
    "            self.q_table = self.import_Qtable(filename)\n",
    "        else:\n",
    "            self.q_table = {} # La tabla que hay que exportar y leer. self.import_Qtable(self.filename)\n",
    "\n",
    "    def import_Qtable(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'rb') as file:\n",
    "                self.q_table = pickle.load(file)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Starting with an empty Q-Table.\")\n",
    "        \n",
    "    def export_Qtable(self,filename):\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(self.q_table, file)\n",
    "    \n",
    "    def get_actual_state(self):\n",
    "        state = []\n",
    "        for column in range(self.env.columns):\n",
    "            vectors =self.env.get_vectors_of_column(column)\n",
    "            max_own = 0\n",
    "            max_other = 0\n",
    "            if(vectors[0] == [] and vectors[1] == [] and vectors[2] == [] and vectors[3] == []):\n",
    "                state.append((-1, -1))\n",
    "            else :\n",
    "                for index, vector in enumerate(vectors):\n",
    "                    result = self.env.verify_vector(vector[0], vector[1], self.chip)\n",
    "                    max_own = max(max_own, result[0])\n",
    "                    max_other = max(max_other, result[0])\n",
    "                state.append((max_own, max_other))\n",
    "        return state\n",
    "\n",
    "    def get_value(self, state, action:int) -> float:\n",
    "        if self.q_table.get((state,action)) == None:\n",
    "            return 0\n",
    "        return self.q_table.get((state,action))\n",
    "    \n",
    "    def best_action(self, state) -> int:\n",
    "        actions = self.env.get_possible_actions()\n",
    "        values = {}\n",
    "        \n",
    "        if len(actions) == 0 :\n",
    "            return -1\n",
    "        \n",
    "        for action in actions:\n",
    "            values[(state,action)] = self.get_value(state, action)\n",
    "            \n",
    "        best_state, best_action = max(values, key=values.get)\n",
    "        \n",
    "        #Se busca si existe mas de una tupla con el mismo q valor\n",
    "        filtered_values = {clave: valor for clave, valor in values.items() if valor == self.get_value(best_state, best_action)}\n",
    "        \n",
    "        # Se selecciona aleatoriamente la mejor accion de las tuplas empatadas\n",
    "        best_tuple, best_action_value = random.choice(list(filtered_values.items()))\n",
    "\n",
    "        return best_tuple[1]\n",
    "    \n",
    "    def choose_action(self, state) -> int:\n",
    "        actions = self.env.get_possible_actions()\n",
    "        action = -1\n",
    "        prob = random.uniform(0,1)\n",
    "        if prob <= self.epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = self.best_action(state)\n",
    "        return action\n",
    "    \n",
    "    def update_values(self, state, action:int, next_state, reward:int) -> None:\n",
    "        #ùëÑ(ùë†,ùëé)=(1‚àíùõº)ùëÑ(ùë†,ùëé)+ùõº[ùëü+ùõæmaxùëé‚Ä≤ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤)]\n",
    "        actual_Q_value = self.get_value(state, action)\n",
    "        next_action = self.best_action(next_state)\n",
    "        next_Q_value = self.get_value(next_state, next_action)\n",
    "        new_Q_value = ((1-self.alpha)*actual_Q_value) + self.alpha * (reward+(self.gamma*next_Q_value))\n",
    "        self.q_table[(state,action)] = new_Q_value\n",
    "        return new_Q_value\n",
    "\n",
    "    def step(self, state, action:int) -> tuple[tuple[int,int],int, bool, str]:\n",
    "        own_neighbors, opponent_neighbors = state[action]\n",
    "        # Aca se define la estrategia a tomar, si es defensiva u ofensiva\n",
    "        own_neighbors_rewards = [0,20,50,200] # Son las recompensas dadas por la cantidad de vecinos propios\n",
    "        opponent_neighbors_rewards = [0,5,15,70] # Son las recompensas dadas por la cantidad de vecinos oponentes\n",
    "\n",
    "        reward = own_neighbors_rewards[own_neighbors] + opponent_neighbors_rewards[opponent_neighbors]\n",
    "        self.env.place_chip(action)\n",
    "        status= self.env.verify_winner(self.chip)\n",
    "        info = ''\n",
    "        if status : \n",
    "            info = 'El estado es terminal'\n",
    "        else :\n",
    "            info = 'El juego continua'\n",
    "        return (reward, status, info)\n",
    "\n",
    "    \n",
    "    def play_turn(self, episode = 1):\n",
    "            \n",
    "        if self.state == []:\n",
    "            self.state = self.get_actual_state()\n",
    "            self.action = self.choose_action(self.state)\n",
    "        else:\n",
    "            next_state = self.get_actual_state()\n",
    "            next_action = self.choose_action(next_state)\n",
    "            self.update_values(self.state, self.action, next_state,self.reward)\n",
    "            self.state, self.action = next_state, next_action\n",
    "\n",
    "        self.reward, done, info = self.step(self.state, self.action)\n",
    "        \n",
    "\n",
    "        if ((episode+1) % 100) == 0:\n",
    "            if self.epsilon > 0.01:\n",
    "                    \n",
    "                self.epsilon -= (self.epsilon*0.1)\n",
    "        \n",
    "    def test_performance(self) -> tuple[dict, dict]:\n",
    "        actions = {}\n",
    "        values = {} \n",
    "        for i in range(self.env.nrows):\n",
    "            for j in range(self.env.ncols):                    \n",
    "                    if not self.env.is_terminal((i,j)):\n",
    "                        action = self.best_action((i,j))\n",
    "                        actions[(i,j)] = action\n",
    "                        values[(i,j)] = self.get_value((i,j), action)\n",
    "        return actions, values\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Hay que definir que estrategia va a tener el agente para terminar de definir su estructura, m√©todos y eso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35066b1a-a746-4a76-b6ef-558c8c7aad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = [\n",
    "            ['-','-','-','-','-','-','-'],\n",
    "            ['-','-','-','-','-','-','-'],\n",
    "            ['-','-','O','-','-','-','-'],\n",
    "            ['-','-','O','-','-','-','-'],\n",
    "            ['O','-','X','X','X','X','O'],\n",
    "            ['-','-','O','O','X','O','X']\n",
    "          ]\n",
    "    agente = Agent(env,'X')\n",
    "    \n",
    "    vector = ['-','X','X','-','X','O']\n",
    "    assert agente.verify_vector(3,vector,'X') == (3,0), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','O','O','-','X','O']\n",
    "    assert agente.verify_vector(3,vector,'X') == (1,2), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','O','O','-','X','O']\n",
    "    assert agente.verify_vector(0,vector,'X') == (0,2), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','O','O','X','X','-']\n",
    "    assert agente.verify_vector(5,vector,'X') == (2,0), \"La funcion no esta retornando los valores correctos\"\n",
    "    \n",
    "    vector = ['-','-','-','-']\n",
    "    assert agente.verify_vector(2,vector,'X') == (0,0), \"La funcion no esta retornando los valores correctos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1562e61-abf4-428c-8f54-0703bca36674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# import pickle\n",
    "\n",
    "# class Connect4Agent:\n",
    "#     def __init__(self, chip, reward_scheme=(0.0, -1.0, 0.5, 1.0), filename=None):\n",
    "#         '''\n",
    "#         chip <string>: Chip to be played by the agent. Must be either \"X\" or \"O\".\n",
    "#         reward_scheme <tuple>: (reward for a non-terminal move, reward for losing, reward for a tied game, reward for winning).\n",
    "#         filename <string>: Filename to load or save the Q-Table.\n",
    "#         '''\n",
    "#         if chip not in [\"X\", \"O\"]:\n",
    "#             raise ValueError(\"chip must be 'X' or 'O'\")\n",
    "        \n",
    "#         self.chip = chip\n",
    "#         self.rewards = reward_scheme\n",
    "#         self.filename = filename\n",
    "#         self.q_table = {}  # Q-Table as a dictionary to store state-action values\n",
    "\n",
    "#         # Load Q-Table if a filename is provided\n",
    "#         if filename:\n",
    "#             self.import_Qtable(filename)\n",
    "\n",
    "#     def get_state_representation(self, board):\n",
    "#         \"\"\"\n",
    "#         Converts the game board into a tuple using booleans and empty spaces.\n",
    "#         Returns:\n",
    "#             A tuple representing the board state where:\n",
    "#             - None: The cell is empty.\n",
    "#             - True: The cell contains the agent's chip.\n",
    "#             - False: The cell contains the opponent's chip.\n",
    "#         \"\"\"\n",
    "#         agent_chip = True\n",
    "#         opponent_chip = False\n",
    "#         state = []\n",
    "\n",
    "#         for row in range(6):\n",
    "#             for col in range(7):\n",
    "#                 if board[row][col] == self.chip:\n",
    "#                     state.append(agent_chip)\n",
    "#                 elif board[row][col] != \"-\":  # \"-\" indicates an empty cell\n",
    "#                     state.append(opponent_chip)\n",
    "#                 else:\n",
    "#                     state.append(None)\n",
    "#         return tuple(state)  # Convert the state to a tuple to use as a key in the Q-Table\n",
    "\n",
    "#     def get_available_actions(self, board):\n",
    "#         \"\"\"\n",
    "#         Returns a list of available columns where a chip can be placed.\n",
    "#         \"\"\"\n",
    "#         return [col for col in range(7) if board[0][col] == \"-\"]  # Check the top cell of each column\n",
    "\n",
    "#     def choose_action(self, board, epsilon=0.1):\n",
    "#         \"\"\"\n",
    "#         Chooses an action using the epsilon-greedy strategy.\n",
    "#         \"\"\"\n",
    "#         state = self.get_state_representation(board)\n",
    "#         available_actions = self.get_available_actions(board)\n",
    "\n",
    "#         if random.random() < epsilon:\n",
    "#             # Explore: choose a random action\n",
    "#             return random.choice(available_actions)\n",
    "#         else:\n",
    "#             # Exploit: choose the action with the highest Q-value\n",
    "#             q_values = [self.q_table.get((state, action), 0.0) for action in available_actions]\n",
    "#             max_q_value = max(q_values)\n",
    "#             best_actions = [action for action, q in zip(available_actions, q_values) if q == max_q_value]\n",
    "#             return random.choice(best_actions)  # Choose randomly among the best actions\n",
    "\n",
    "#     def update_q_table(self, board, action, reward, next_board, alpha=0.1, gamma=0.9):\n",
    "#         \"\"\"\n",
    "#         Updates the Q-Table using the Q-Learning formula.\n",
    "#         \"\"\"\n",
    "#         state = self.get_state_representation(board)\n",
    "#         next_state = self.get_state_representation(next_board)\n",
    "#         next_available_actions = self.get_available_actions(next_board)\n",
    "\n",
    "#         # Current Q-value\n",
    "#         current_q_value = self.q_table.get((state, action), 0.0)\n",
    "\n",
    "#         # Max Q-value for the next state\n",
    "#         if next_available_actions:\n",
    "#             next_q_values = [self.q_table.get((next_state, next_action), 0.0) for next_action in next_available_actions]\n",
    "#             max_next_q_value = max(next_q_values)\n",
    "#         else:\n",
    "#             max_next_q_value = 0.0  # No future actions if the game is over\n",
    "\n",
    "#         # Q-Learning update\n",
    "#         new_q_value = current_q_value + alpha * (reward + gamma * max_next_q_value - current_q_value)\n",
    "#         self.q_table[(state, action)] = new_q_value\n",
    "\n",
    "#     def import_Qtable(self, filename):\n",
    "#         \"\"\"\n",
    "#         Imports the Q-Table from a file.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             with open(filename, 'rb') as file:\n",
    "#                 self.q_table = pickle.load(file)\n",
    "#         except FileNotFoundError:\n",
    "#             print(\"File not found. Starting with an empty Q-Table.\")\n",
    "\n",
    "#     def export_Qtable(self, filename):\n",
    "#         \"\"\"\n",
    "#         Exports the Q-Table to a file.\n",
    "#         \"\"\"\n",
    "#         with open(filename, 'wb') as file:\n",
    "#             pickle.dump(self.q_table, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46648c-4a8b-4910-b57d-83fc97f1aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crear una instancia del agente\n",
    "# agent = Connect4Agent(chip=\"X\", reward_scheme=(0.0, -1.0, 0.5, 1.0))\n",
    "\n",
    "# # Ejemplo de un tablero de juego\n",
    "# board = [\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"X\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"X\", \"O\", \"-\", \"-\"],\n",
    "#     [\"X\", \"X\", \"X\", \"O\", \"X\", \"-\", \"-\"],\n",
    "#     [\"O\", \"X\", \"O\", \"X\", \"O\", \"-\", \"-\"]\n",
    "# ]\n",
    "\n",
    "# # Escoger una acci√≥n con un 10% de exploraci√≥n y 90% de explotaci√≥n\n",
    "# epsilon = 0.1  # Tasa de exploraci√≥n\n",
    "# action = agent.choose_action(board, epsilon)\n",
    "# print(\"Chosen action (column):\", action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
