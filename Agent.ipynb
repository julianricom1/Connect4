{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "caca4075-ec01-443d-b49b-b84f0ea798aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition on the Agent that plays Connect4\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nbimporter\n",
    "from Board import Connect4Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "68cc4297-5b22-4c9d-99aa-213f94b01ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env,chip, reward_scheme = (0.0, -1.0, 0.5, 1.0), filename = None):\n",
    "        '''\n",
    "        reward_scheme <(float, float, float, float)> : (reward for a move that doesn´t end the game, reward for losing, reward fora tied game, reward for winning)\n",
    "        chip <string> = chip to be played by the agent. must be either \"X\" or \"O\"\n",
    "        \n",
    "        '''\n",
    "        self.env = np.array(env)\n",
    "        \n",
    "        # Verificar que reward_scheme es una tupla de 4 floats\n",
    "        if not (isinstance(reward_scheme, tuple) and len(reward_scheme) == 4 and all(isinstance(x, float) for x in reward_scheme)):\n",
    "            raise ValueError(\"reward_scheme must be a tupla with 4 floats\")\n",
    "        self.rewards = reward_scheme\n",
    "     \n",
    "        if chip not in [\"X\", \"O\"]:\n",
    "            raise ValueError(\"chip must be 'X' or 'O'\")\n",
    "        self.chip = chip\n",
    "        self.values = None #TODO\n",
    "        self.filename = filename  # Nuevo parámetro opcional\n",
    "        self.Q_table = {} # La tabla que hay que exportar y leer. self.import_Qtable(self.filename)\n",
    "\n",
    "    def import_Qtable(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'rb') as file:\n",
    "                self.q_table = pickle.load(file)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Starting with an empty Q-Table.\")\n",
    "        \n",
    "    def export_Qtable(self,filename):\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(self.q_table, file)\n",
    "\n",
    "    def get_vectors_of_column(self,column):\n",
    "        #deberia ser env.board - se continua por temas de desarrollo\n",
    "        available_row = self.get_next_available(column)\n",
    "        if available_row == -1:\n",
    "            return [], [], [], []\n",
    "        \n",
    "        reference_matrix = self.env\n",
    "        reference_matrix[available_row, column] = 'A'\n",
    "        horizontal_vector = reference_matrix[available_row, 0:7 ]\n",
    "        vertical_vector = reference_matrix[0:6, column]\n",
    "        \n",
    "        diagonal_1_vector =  reference_matrix.diagonal(column-available_row)\n",
    "        diagonal_2_vector = reference_matrix[::-1].diagonal(column+(available_row-5))\n",
    "        return (column, horizontal_vector), (available_row,vertical_vector), (np.where(diagonal_1_vector == 'A')[0][0],diagonal_1_vector), (np.where(diagonal_2_vector == 'A')[0][0],diagonal_2_vector)\n",
    "\n",
    "    #esta funcion debe estar en el board\n",
    "    def get_next_available(self,column):\n",
    "        #deberia ser self.env.board - se continua por temas de desarrollo\n",
    "        for row in reversed(range(6)):\n",
    "            if self.env[row, column] == \"-\":\n",
    "                return row\n",
    "        return -1\n",
    "\n",
    "    def verify_vector(self, actual_position, vector, chip_type):\n",
    "\n",
    "        counter_chip_type = 0\n",
    "        counter_other = 0\n",
    "        \n",
    "        if actual_position < len(vector)-1 :  \n",
    "            for p1 in range(actual_position+1, len(vector)):\n",
    "                if vector[actual_position+1] == vector[p1]:\n",
    "                    if vector[p1] == chip_type:\n",
    "                        counter_chip_type += 1\n",
    "                    if vector[p1] != chip_type and vector[p1] != '-':\n",
    "                        counter_other += 1\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "        if actual_position > 0 :\n",
    "            for p2 in range(actual_position-1, 0, -1):\n",
    "                if vector[actual_position-1] == vector[p2]:\n",
    "                    if vector[p2] == chip_type:\n",
    "                        counter_chip_type += 1\n",
    "                    if vector[p2] != chip_type and vector[p2] != '-':\n",
    "                        counter_other += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "        return counter_chip_type, counter_other\n",
    "    \n",
    "    def get_actual_state(self):\n",
    "        state = []\n",
    "        for column in range(self.env.columns):\n",
    "            vectors =self.get_vectors_of_column(column)\n",
    "            max_own = 0\n",
    "            max_other = 0\n",
    "            for index, vector in enumerate(vectors):\n",
    "                result = self.verify_vector(vector[0], vector[1], self.chip)\n",
    "                max_own = max(max_own, result[0])\n",
    "                max_other = max(max_other, result[0])\n",
    "            state.append((max_own, max_other))\n",
    "        return state\n",
    "        \n",
    "\n",
    "# Hay que definir que estrategia va a tener el agente para terminar de definir su estructura, métodos y eso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b2915fc5-aabc-4e51-81a9-ccb3049c802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, array(['O', 'X', 'A', '-', '-'], dtype='<U1'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "horizontal_vector, vertical_vector, diagonal_1_vector, diagonal_2_vector = agente.get_vectors_of_column(4)\n",
    "print(diagonal_2_vector)\n",
    "#assert agente.get_vectors_of_column(3) == (3,0), \"La funcion no esta retornando los valores correctos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "635888b9-0854-4dc8-b5f4-6444c4ec0972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal principal: [29 23 17 11  5]\n"
     ]
    }
   ],
   "source": [
    "# Crear una matriz de ejemplo\n",
    "matriz = np.array([[1, 2, 3,  4, 5, 6, 7], \n",
    "                   [8, 9, 10,11,12,13,14], \n",
    "                   [15,16,17,18,19,20,21],\n",
    "                   [22,23,24,25,26,27,28],\n",
    "                   [29,30,31,32,33,34,35],\n",
    "                   [36,37,38,39,40,41,42]])\n",
    "\n",
    "# Diagonal principal (k=0)\n",
    "diagonal_principal = matriz[::-1].diagonal(-1)\n",
    "print(\"Diagonal principal:\", diagonal_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35066b1a-a746-4a76-b6ef-558c8c7aad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = [\n",
    "        ['-','-','-','-','-','-','-'],\n",
    "        ['-','-','-','-','-','-','-'],\n",
    "        ['-','-','O','-','-','-','-'],\n",
    "        ['-','-','O','-','-','-','-'],\n",
    "        ['O','-','X','X','X','X','O'],\n",
    "        ['-','-','O','O','X','O','X']\n",
    "      ]\n",
    "agente = Agent(env,'X')\n",
    "\n",
    "vector = ['-','X','X','-','X','O']\n",
    "assert agente.verify_vector(3,vector,'X') == (3,0), \"La funcion no esta retornando los valores correctos\"\n",
    "\n",
    "vector = ['-','O','O','-','X','O']\n",
    "assert agente.verify_vector(3,vector,'X') == (1,2), \"La funcion no esta retornando los valores correctos\"\n",
    "\n",
    "vector = ['-','O','O','-','X','O']\n",
    "assert agente.verify_vector(0,vector,'X') == (0,2), \"La funcion no esta retornando los valores correctos\"\n",
    "\n",
    "vector = ['-','O','O','X','X','-']\n",
    "assert agente.verify_vector(5,vector,'X') == (2,0), \"La funcion no esta retornando los valores correctos\"\n",
    "\n",
    "vector = ['-','-','-','-']\n",
    "assert agente.verify_vector(2,vector,'X') == (0,0), \"La funcion no esta retornando los valores correctos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1562e61-abf4-428c-8f54-0703bca36674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# import pickle\n",
    "\n",
    "# class Connect4Agent:\n",
    "#     def __init__(self, chip, reward_scheme=(0.0, -1.0, 0.5, 1.0), filename=None):\n",
    "#         '''\n",
    "#         chip <string>: Chip to be played by the agent. Must be either \"X\" or \"O\".\n",
    "#         reward_scheme <tuple>: (reward for a non-terminal move, reward for losing, reward for a tied game, reward for winning).\n",
    "#         filename <string>: Filename to load or save the Q-Table.\n",
    "#         '''\n",
    "#         if chip not in [\"X\", \"O\"]:\n",
    "#             raise ValueError(\"chip must be 'X' or 'O'\")\n",
    "        \n",
    "#         self.chip = chip\n",
    "#         self.rewards = reward_scheme\n",
    "#         self.filename = filename\n",
    "#         self.q_table = {}  # Q-Table as a dictionary to store state-action values\n",
    "\n",
    "#         # Load Q-Table if a filename is provided\n",
    "#         if filename:\n",
    "#             self.import_Qtable(filename)\n",
    "\n",
    "#     def get_state_representation(self, board):\n",
    "#         \"\"\"\n",
    "#         Converts the game board into a tuple using booleans and empty spaces.\n",
    "#         Returns:\n",
    "#             A tuple representing the board state where:\n",
    "#             - None: The cell is empty.\n",
    "#             - True: The cell contains the agent's chip.\n",
    "#             - False: The cell contains the opponent's chip.\n",
    "#         \"\"\"\n",
    "#         agent_chip = True\n",
    "#         opponent_chip = False\n",
    "#         state = []\n",
    "\n",
    "#         for row in range(6):\n",
    "#             for col in range(7):\n",
    "#                 if board[row][col] == self.chip:\n",
    "#                     state.append(agent_chip)\n",
    "#                 elif board[row][col] != \"-\":  # \"-\" indicates an empty cell\n",
    "#                     state.append(opponent_chip)\n",
    "#                 else:\n",
    "#                     state.append(None)\n",
    "#         return tuple(state)  # Convert the state to a tuple to use as a key in the Q-Table\n",
    "\n",
    "#     def get_available_actions(self, board):\n",
    "#         \"\"\"\n",
    "#         Returns a list of available columns where a chip can be placed.\n",
    "#         \"\"\"\n",
    "#         return [col for col in range(7) if board[0][col] == \"-\"]  # Check the top cell of each column\n",
    "\n",
    "#     def choose_action(self, board, epsilon=0.1):\n",
    "#         \"\"\"\n",
    "#         Chooses an action using the epsilon-greedy strategy.\n",
    "#         \"\"\"\n",
    "#         state = self.get_state_representation(board)\n",
    "#         available_actions = self.get_available_actions(board)\n",
    "\n",
    "#         if random.random() < epsilon:\n",
    "#             # Explore: choose a random action\n",
    "#             return random.choice(available_actions)\n",
    "#         else:\n",
    "#             # Exploit: choose the action with the highest Q-value\n",
    "#             q_values = [self.q_table.get((state, action), 0.0) for action in available_actions]\n",
    "#             max_q_value = max(q_values)\n",
    "#             best_actions = [action for action, q in zip(available_actions, q_values) if q == max_q_value]\n",
    "#             return random.choice(best_actions)  # Choose randomly among the best actions\n",
    "\n",
    "#     def update_q_table(self, board, action, reward, next_board, alpha=0.1, gamma=0.9):\n",
    "#         \"\"\"\n",
    "#         Updates the Q-Table using the Q-Learning formula.\n",
    "#         \"\"\"\n",
    "#         state = self.get_state_representation(board)\n",
    "#         next_state = self.get_state_representation(next_board)\n",
    "#         next_available_actions = self.get_available_actions(next_board)\n",
    "\n",
    "#         # Current Q-value\n",
    "#         current_q_value = self.q_table.get((state, action), 0.0)\n",
    "\n",
    "#         # Max Q-value for the next state\n",
    "#         if next_available_actions:\n",
    "#             next_q_values = [self.q_table.get((next_state, next_action), 0.0) for next_action in next_available_actions]\n",
    "#             max_next_q_value = max(next_q_values)\n",
    "#         else:\n",
    "#             max_next_q_value = 0.0  # No future actions if the game is over\n",
    "\n",
    "#         # Q-Learning update\n",
    "#         new_q_value = current_q_value + alpha * (reward + gamma * max_next_q_value - current_q_value)\n",
    "#         self.q_table[(state, action)] = new_q_value\n",
    "\n",
    "#     def import_Qtable(self, filename):\n",
    "#         \"\"\"\n",
    "#         Imports the Q-Table from a file.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             with open(filename, 'rb') as file:\n",
    "#                 self.q_table = pickle.load(file)\n",
    "#         except FileNotFoundError:\n",
    "#             print(\"File not found. Starting with an empty Q-Table.\")\n",
    "\n",
    "#     def export_Qtable(self, filename):\n",
    "#         \"\"\"\n",
    "#         Exports the Q-Table to a file.\n",
    "#         \"\"\"\n",
    "#         with open(filename, 'wb') as file:\n",
    "#             pickle.dump(self.q_table, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc46648c-4a8b-4910-b57d-83fc97f1aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action (column): 5\n"
     ]
    }
   ],
   "source": [
    "# # Crear una instancia del agente\n",
    "# agent = Connect4Agent(chip=\"X\", reward_scheme=(0.0, -1.0, 0.5, 1.0))\n",
    "\n",
    "# # Ejemplo de un tablero de juego\n",
    "# board = [\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"X\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"X\", \"O\", \"-\", \"-\"],\n",
    "#     [\"X\", \"X\", \"X\", \"O\", \"X\", \"-\", \"-\"],\n",
    "#     [\"O\", \"X\", \"O\", \"X\", \"O\", \"-\", \"-\"]\n",
    "# ]\n",
    "\n",
    "# # Escoger una acción con un 10% de exploración y 90% de explotación\n",
    "# epsilon = 0.1  # Tasa de exploración\n",
    "# action = agent.choose_action(board, epsilon)\n",
    "# print(\"Chosen action (column):\", action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
