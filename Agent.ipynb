{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca4075-ec01-443d-b49b-b84f0ea798aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition on the Agent that plays Connect4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68cc4297-5b22-4c9d-99aa-213f94b01ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init_(self, chip, reward_scheme = (0.0, -1.0, 0.5, 1.0), filename = None):\n",
    "        '''\n",
    "        reward_scheme <(float, float, float, float)> : (reward for a move that doesn´t end the game, reward for losing, reward fora tied game, reward for winning)\n",
    "        chip <string> = chip to be played by the agent. must be either \"X\" or \"O\"\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Verificar que reward_scheme es una tupla de 4 floats\n",
    "        if not (isinstance(reward_scheme, tuple) and len(reward_scheme) == 4 and all(isinstance(x, float) for x in reward_scheme)):\n",
    "            raise ValueError(\"reward_scheme must be a tupla with 4 floats\")\n",
    "        self.rewards = reward_scheme\n",
    "     \n",
    "        if chip not in [\"X\", \"O\"]:\n",
    "            raise ValueError(\"chip must be 'X' or 'O'\")\n",
    "        self.chip = chip\n",
    "        self.values = None #TODO\n",
    "        self.filename = filename  # Nuevo parámetro opcional\n",
    "        #self.Q_table = ? # La tabla que hay que exportar y leer. self.import_Qtable(self.filename)\n",
    "\n",
    "    def import_Qtable(self,filename):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def export_Qtable(self,filename):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "# Hay que definir que estrategia va a tener el agente para terminar de definir su estructura, métodos y eso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1562e61-abf4-428c-8f54-0703bca36674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# import pickle\n",
    "\n",
    "# class Connect4Agent:\n",
    "#     def __init__(self, chip, reward_scheme=(0.0, -1.0, 0.5, 1.0), filename=None):\n",
    "#         '''\n",
    "#         chip <string>: Chip to be played by the agent. Must be either \"X\" or \"O\".\n",
    "#         reward_scheme <tuple>: (reward for a non-terminal move, reward for losing, reward for a tied game, reward for winning).\n",
    "#         filename <string>: Filename to load or save the Q-Table.\n",
    "#         '''\n",
    "#         if chip not in [\"X\", \"O\"]:\n",
    "#             raise ValueError(\"chip must be 'X' or 'O'\")\n",
    "        \n",
    "#         self.chip = chip\n",
    "#         self.rewards = reward_scheme\n",
    "#         self.filename = filename\n",
    "#         self.q_table = {}  # Q-Table as a dictionary to store state-action values\n",
    "\n",
    "#         # Load Q-Table if a filename is provided\n",
    "#         if filename:\n",
    "#             self.import_Qtable(filename)\n",
    "\n",
    "#     def get_state_representation(self, board):\n",
    "#         \"\"\"\n",
    "#         Converts the game board into a tuple using booleans and empty spaces.\n",
    "#         Returns:\n",
    "#             A tuple representing the board state where:\n",
    "#             - None: The cell is empty.\n",
    "#             - True: The cell contains the agent's chip.\n",
    "#             - False: The cell contains the opponent's chip.\n",
    "#         \"\"\"\n",
    "#         agent_chip = True\n",
    "#         opponent_chip = False\n",
    "#         state = []\n",
    "\n",
    "#         for row in range(6):\n",
    "#             for col in range(7):\n",
    "#                 if board[row][col] == self.chip:\n",
    "#                     state.append(agent_chip)\n",
    "#                 elif board[row][col] != \"-\":  # \"-\" indicates an empty cell\n",
    "#                     state.append(opponent_chip)\n",
    "#                 else:\n",
    "#                     state.append(None)\n",
    "#         return tuple(state)  # Convert the state to a tuple to use as a key in the Q-Table\n",
    "\n",
    "#     def get_available_actions(self, board):\n",
    "#         \"\"\"\n",
    "#         Returns a list of available columns where a chip can be placed.\n",
    "#         \"\"\"\n",
    "#         return [col for col in range(7) if board[0][col] == \"-\"]  # Check the top cell of each column\n",
    "\n",
    "#     def choose_action(self, board, epsilon=0.1):\n",
    "#         \"\"\"\n",
    "#         Chooses an action using the epsilon-greedy strategy.\n",
    "#         \"\"\"\n",
    "#         state = self.get_state_representation(board)\n",
    "#         available_actions = self.get_available_actions(board)\n",
    "\n",
    "#         if random.random() < epsilon:\n",
    "#             # Explore: choose a random action\n",
    "#             return random.choice(available_actions)\n",
    "#         else:\n",
    "#             # Exploit: choose the action with the highest Q-value\n",
    "#             q_values = [self.q_table.get((state, action), 0.0) for action in available_actions]\n",
    "#             max_q_value = max(q_values)\n",
    "#             best_actions = [action for action, q in zip(available_actions, q_values) if q == max_q_value]\n",
    "#             return random.choice(best_actions)  # Choose randomly among the best actions\n",
    "\n",
    "#     def update_q_table(self, board, action, reward, next_board, alpha=0.1, gamma=0.9):\n",
    "#         \"\"\"\n",
    "#         Updates the Q-Table using the Q-Learning formula.\n",
    "#         \"\"\"\n",
    "#         state = self.get_state_representation(board)\n",
    "#         next_state = self.get_state_representation(next_board)\n",
    "#         next_available_actions = self.get_available_actions(next_board)\n",
    "\n",
    "#         # Current Q-value\n",
    "#         current_q_value = self.q_table.get((state, action), 0.0)\n",
    "\n",
    "#         # Max Q-value for the next state\n",
    "#         if next_available_actions:\n",
    "#             next_q_values = [self.q_table.get((next_state, next_action), 0.0) for next_action in next_available_actions]\n",
    "#             max_next_q_value = max(next_q_values)\n",
    "#         else:\n",
    "#             max_next_q_value = 0.0  # No future actions if the game is over\n",
    "\n",
    "#         # Q-Learning update\n",
    "#         new_q_value = current_q_value + alpha * (reward + gamma * max_next_q_value - current_q_value)\n",
    "#         self.q_table[(state, action)] = new_q_value\n",
    "\n",
    "#     def import_Qtable(self, filename):\n",
    "#         \"\"\"\n",
    "#         Imports the Q-Table from a file.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             with open(filename, 'rb') as file:\n",
    "#                 self.q_table = pickle.load(file)\n",
    "#         except FileNotFoundError:\n",
    "#             print(\"File not found. Starting with an empty Q-Table.\")\n",
    "\n",
    "#     def export_Qtable(self, filename):\n",
    "#         \"\"\"\n",
    "#         Exports the Q-Table to a file.\n",
    "#         \"\"\"\n",
    "#         with open(filename, 'wb') as file:\n",
    "#             pickle.dump(self.q_table, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc46648c-4a8b-4910-b57d-83fc97f1aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action (column): 5\n"
     ]
    }
   ],
   "source": [
    "# # Crear una instancia del agente\n",
    "# agent = Connect4Agent(chip=\"X\", reward_scheme=(0.0, -1.0, 0.5, 1.0))\n",
    "\n",
    "# # Ejemplo de un tablero de juego\n",
    "# board = [\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"-\", \"X\", \"-\", \"-\"],\n",
    "#     [\"-\", \"-\", \"-\", \"X\", \"O\", \"-\", \"-\"],\n",
    "#     [\"X\", \"X\", \"X\", \"O\", \"X\", \"-\", \"-\"],\n",
    "#     [\"O\", \"X\", \"O\", \"X\", \"O\", \"-\", \"-\"]\n",
    "# ]\n",
    "\n",
    "# # Escoger una acción con un 10% de exploración y 90% de explotación\n",
    "# epsilon = 0.1  # Tasa de exploración\n",
    "# action = agent.choose_action(board, epsilon)\n",
    "# print(\"Chosen action (column):\", action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
